{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1 HW\n",
    "\n",
    "\n",
    "1. This assignment is a group effort.\n",
    "2. Submission to be uploaded into your group repositories in the folder week1\n",
    "3. Deadline is 27th of April 5:00 PM.\n",
    "4. Please follow google's [python styleguide](https://google.github.io/styleguide/pyguide.html) for your code. Pay attention to the guidelines for naming convention, comments and main.\n",
    "5. Code will be checked for plagiarism. Compelling signs of a duplicated effort will lead to a rejection of submission and will attract a 100\\% grade penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the template provided as a starting point. Extend the classes as you see fit. Be careful to place new attributes and methods in the approriate class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "Extend the classes to include the following methods\n",
    "\n",
    "1. document_term_matrix - which returns a D by V array of frequency counts.\n",
    "2. tf_idf - returns a D by V array of tf-idf scores\n",
    "3. dict_rank - returns the top `n` documents based on a given dictionary and represenation of tokens (eg. doc-term matrix or tf-idf matrix)  \n",
    "\n",
    "Include subroutines as and when necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# In my environment, a harmless exception is thrown from the following\n",
    "# import.  Just ignore it.\n",
    "try:\n",
    "    from nltk.tokenize import wordpunct_tokenize\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from math import log\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document():    \n",
    "    \"\"\"\n",
    "    The Doc class represents a class of individual documents\n",
    "    \"\"\"    \n",
    "    def __init__(self, speech_year, speech_pres, speech_text):\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.text = speech_text.lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        \n",
    "    def token_clean(self,length):\n",
    "        \"\"\" \n",
    "        Description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "        \"\"\"\n",
    "        Description: remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "    def stem(self):\n",
    "        \"\"\"\n",
    "        Description: stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n",
    "\n",
    "    def term_vector(self, doc_token_list):\n",
    "        vector = [None] * len(doc_token_list)\n",
    "        counter = Counter(self.tokens)\n",
    "        for i in range(len(doc_token_list)):\n",
    "            count = counter[doc_token_list[i]]\n",
    "            vector[i] = count\n",
    "\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus():\n",
    "    \"\"\"\n",
    "    The Corpus class represents a document collection.\n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length):\n",
    "        \"\"\"\n",
    "        Notice that the __init__ method is invoked everytime an object of the\n",
    "        class is instantiated.\n",
    "        \"\"\"\n",
    "        # Initialise documents by invoking the appropriate class\n",
    "        self.docs = [Document(doc[0], doc[1], doc[2]) for doc in doc_data]         \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        # Get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "        \n",
    "        # Stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(2)\n",
    "        \n",
    "        # Create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs.\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        Description: parses a file of stopwords, removes words of length\n",
    "        'length' and  stems it.\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"        \n",
    "        with codecs.open(stopword_file, 'r', 'utf-8') as f: raw = f.read()        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "             \n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        Description: create a set of all all tokens or in other words a\n",
    "        vocabulary\n",
    "        \"\"\"       \n",
    "        # Initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "    \n",
    "    def document_term_matrix(self):\n",
    "        result = []\n",
    "        for doc in self.docs:\n",
    "            vector = doc.term_vector(list(self.token_set))\n",
    "            result.append(vector)        \n",
    "        \n",
    "        return result\n",
    "\n",
    "    def tf_idf(self):\n",
    "        dt_matrix = self.document_term_matrix()\n",
    "        tf_matrix = []\n",
    "        idf_matrix = []\n",
    "        tf_idf_matrix = []\n",
    "\n",
    "        # Build a term frequency matrix from the document term matrix.\n",
    "        # tf(d,v) = { 0 if x(d,v) = 0, 1 + log(x(d), v) otherwise }\n",
    "        for dt_doc_vector in dt_matrix:\n",
    "            tf_doc_vector = [(0 if x == 0 else 1 + log(x)) for x in dt_doc_vector]\n",
    "            tf_matrix.append(tf_doc_vector)\n",
    "\n",
    "        # Build a document frequency matrix for each term.\n",
    "        # Initialize with zeros.\n",
    "        df_vector = [0] * len(self.token_set)\n",
    "        for dt_doc_vector in dt_matrix:\n",
    "            # Increment the counters based on an indicator function which\n",
    "            # is 1 if there is at least one instance of the term in the doc.\n",
    "            df_vector = np.add(df_vector, [int(x > 0) for x in dt_doc_vector])\n",
    "\n",
    "        # Build an inverse document frequency vector.\n",
    "        idf_doc_vector = [log(len(self.docs) / x) for x in df_vector]\n",
    "\n",
    "        # Build the TF-IDF weighting matrix.\n",
    "        for tf_doc_vector in tf_matrix:\n",
    "            tf_idf_vector = np.multiply(tf_doc_vector, idf_doc_vector)\n",
    "            tf_idf_matrix.append(tf_idf_vector)\n",
    "\n",
    "        return tf_idf_matrix\n",
    "\n",
    "    def dict_rank(self, dictionary, n):        \n",
    "        dtm = self.document_term_matrix()\n",
    "        all_tokens = list(self.token_set)\n",
    "        \n",
    "        # Get rid of words in the document term matrix not in the dictionary\n",
    "        vec_positions = [0] * len(dtm[0])        \n",
    "        for i in range(len(all_tokens)):\n",
    "            if all_tokens[i] in dictionary:\n",
    "                vec_positions[i] = 1\n",
    "            else:\n",
    "                vec_positions[i] = 0\n",
    "        sums = [0] * len(dtm)\n",
    "\n",
    "        # Get the score of each document\n",
    "        for j in range(len(dtm)):\n",
    "            sums[j] = sum([a * b for a, b in zip(dtm[j], vec_positions)])\n",
    "\n",
    "        # Order them and return the n top documents\n",
    "        order = sorted(range(len(sums)), key = lambda k: sums[k])\n",
    "        ordered_doc_data_n = [0] * len(dtm)\n",
    "        counter = 0        \n",
    "        for num in order:\n",
    "            ordered_doc_data_n[counter] = doc_data[num]\n",
    "            counter += 1\n",
    "        n_top = ordered_doc_data_n[0:n]\n",
    "       \n",
    "        return n_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_text(textraw, regex):\n",
    "    \"\"\"\n",
    "    Takes raw string and performs two operations:\n",
    "      1. Breaks text into a list of speech, president and speech\n",
    "      2. Breaks speech into paragraphs\n",
    "    \"\"\"\n",
    "    prs_yr_spch_reg = re.compile(regex, re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "    # Each tuple contains the year, name of the president and the speech text\n",
    "    prs_yr_spch = prs_yr_spch_reg.findall(textraw)\n",
    "    \n",
    "    # Convert immutabe tuple to mutable list\n",
    "    prs_yr_spch = [list(tup) for tup in prs_yr_spch]\n",
    "    for i in range(len(prs_yr_spch)):\n",
    "        prs_yr_spch[i][2] = prs_yr_spch[i][2].replace('\\n', '')\n",
    "    \n",
    "    # Sort\n",
    "    prs_yr_spch.sort()\n",
    "    \n",
    "    return prs_yr_spch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Corpus instance at 0x1077f95f0>\n",
      "<__main__.Document instance at 0x1077f9878>\n",
      "[array([ 1.09861229,  0.        ,  0.        , ...,  0.        ,\n",
      "        0.        ,  0.        ]), array([ 0.        ,  0.        ,  1.09861229, ...,  1.09861229,\n",
      "        1.09861229,  0.        ]), array([ 0.        ,  1.09861229,  0.        , ...,  0.        ,\n",
      "        0.        ,  0.        ])]\n"
     ]
    }
   ],
   "source": [
    "text = open('./../data/pres_speech/sou_all.txt', 'r').read()\n",
    "regex = '_(\\d{4}).*?_[a-zA-Z]+.*?_[a-zA-Z]+.*?_([a-zA-Z]+)_\\*+(\\\\n{2}.*?)\\\\n{3}'\n",
    "pres_speech_list = parse_text(text, regex)\n",
    "\n",
    "# Instantiate the corpus class\n",
    "corpus = Corpus(pres_speech_list, './../data/stopwords/stopwords.txt', 2)\n",
    "tf_idf = corpus.tf_idf()\n",
    "\n",
    "print corpus\n",
    "print corpus.docs[0]\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "Pick a dictionary (or dictionaries) of your choice from the Harvard IV set, the Loughran-McDonald set, or some other of your choosing that you think may be relevant for the data you collected. Then conduct the following exercise:\n",
    "1. Use the two methods above to score each document in your data.\n",
    "2. Explore whether the scores diﬀer according to the meta data ﬁelds you gathered: for example, do diﬀerent speakers/sources/etc tend to receive a higher score than others?\n",
    "3. Do the answers to the previous question depend on whether tf-idf weighting is applied or not? Why do you think there is (or is not) a diﬀerence in your answers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Harvard IV set\n",
    "file_handler = './../data/dictionary/inquirerbasic2.csv'\n",
    "dictionary = np.loadtxt(open(file_handler, 'rb'), dtype = 'str',\n",
    "                        delimiter = ';', skiprows = 1, comments = None)\n",
    "\n",
    "type(dictionary)\n",
    "#len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "We will now do a sentiment analysis using the AFINN list of words. AFINN is a list of English words rated for valence with an integer between minus five (negative) and plus five (positive). The words have been manually labeled by Finn Årup Nielsen in 2009-2011. A positive valence score can be interpreted as the word conveying a postive emotion and vice versa. \n",
    "\n",
    "Load _AFINN-111.txt_ from ./data/AFINN. Inspect the contents of the file and write a function that converts it into a dictionary where the keys are words and values are the valence scores attributed to them. You may use the readme file for hints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "Now, use the presedential speeches from last week's HW to calculate its sentiment score. Match every word against the dictionary and come up with a metric that captures the sentiment value. If a word is not present mark its score as 0. Write a function that takes in a list of word and returns their sentiment score. What is the score of the speech you have been assigned? Which year, president gave the least and most positive speech?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
